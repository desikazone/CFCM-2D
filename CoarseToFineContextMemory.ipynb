{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "CoarseToFineContextMemory.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/desikazone/CFCM-2D/blob/master/CoarseToFineContextMemory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcJZ25-ikKIa",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://www.nvidia.com/dli\"> <img src=\"DLI Header.png\" alt=\"Header\" style=\"width: 400px;\"/> </a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUmmqSSVkKId",
        "colab_type": "text"
      },
      "source": [
        "# Lung segmentation in X-Ray images with Coarse to Fine Context Memory\n",
        "\n",
        "#  <span style=\"color:green\">Overview</span>\n",
        "\n",
        "Medical Image segmentation is an essential component of various medical imaging analysis solutions. By partitioning a medical image into disjunct segments, it is for example possible to quantify the extent of a tumor or to study the anatomical structure of a region of interest. But it is also crucial as a first step for subsequent algorithms: by locating the region of interest, the search space for computational extensive algorithms can significantly be reduced. And these are only a few examples of the wide range of applications for segmentation. \n",
        "\n",
        "Although there has been significant progress in this active area of research, there is no unique solution for this challenging task. In recent years, approaches based on deep learning have proven to be particularly successful and an architecture that has widely been used is the Encoding-Decoding approach. \n",
        "\n",
        "In this lab we will investigate this popular architecture and discuss several strategies of fusing features from the encoding path with the decoding path. In the second part, we will highlight some of the best principles for structuring code for deep learning projects. Finally, we will see how the fusion of features can be further improved with the example of a recent architecture published at MICCAI and lung segmentation in x-ray images. However, it should be noted that the approach is general and can be applied for various medical image types. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63kgx26TkKIg",
        "colab_type": "text"
      },
      "source": [
        "#  <span style=\"color:green\">Introduction</span>\n",
        "## The importance of skip connections in biomedical image segmentation\n",
        "In a medical segmentation task, we want to map every pixel location of a medical image to a distinct class value representing for example background or tumor. Consequently, the input (image) and the output (segmentation) usually have the same spatial extent. A straight-forward approach to design a neural network architecture to achieve this goal would be to have several fully connected layers without changing the spatial dimensionality. However, this approach would very quickly lead to an explosion in terms of number of parameters. Convolutional operations have the advantage to significantly reduce the number of parameters, as the same operation is applied in strides over the entire image. Furthermore they are invariant to translation, e.g. it is not important if the object of interest is shifted. \n",
        "\n",
        "Designing a fully convolutional network without reducing the spatial dimensions would be possible, but research indicates that a encoder-decoder network is more effective [Ref. 6]. An encoder-decoder network typically looks like this:\n",
        "\n",
        "![Encoder-Decoder Network](encoder.png)\n",
        "\n",
        "The contracting path (Encoder) maps the image to a feature representation that is often lower dimensional than the original input size. The expanding path (Decoder) maps the feature representation to the output space. Originally, this type of network was known in connection with auto-encoder, where the output space is the same as the input space. In case of segmentation the output space often has the same spatial extend as the input space, but represents different content (classes). \n",
        "\n",
        "Thanks to the downsampling in the contracting path, fewer parameters need to be trained. However, this comes at the cost of losing spatial information. One approach to counteract this is the use of so-called skip connections: allowing the gradient to skip part of the network and to flow directly from a layer of the contracting part to the expanding path. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M76QGVoZkKIi",
        "colab_type": "text"
      },
      "source": [
        "## Fusing features from different layers\n",
        "There are different techniques to realize these skip connections. Two of the most common ones are concatenation and summation:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f9s72XVkKIk",
        "colab_type": "text"
      },
      "source": [
        "### Feature concatenation\n",
        "One possibility is to simply concatenate the layers. This requires the layers to have the same dimension in the concatenation direction. \n",
        "So a concatenation t1 = [1 2 3] with t2 = [4 5 6] could be t_new = [1 2 3 4 5 6].  In Tensorflow this operation is tf.concat ([see here](https://www.tensorflow.org/api_docs/python/tf/concat)).\n",
        "\n",
        "<img src=concatenation.png width=\"600\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os31QBiTkKIl",
        "colab_type": "text"
      },
      "source": [
        "### Feature summation\n",
        "Another widely used approach is element-wise summation. One very nice property is that it keeps the number of features fixed. A summation of t1= [1 2 3] and t2 = [4 5 6] would be t_new = [5 7 9]. \n",
        "<img src=summation.png width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLzAcgxFkKIn",
        "colab_type": "text"
      },
      "source": [
        "### Coarse to Fine Context Memory (CFCM)\n",
        "The combination of layers is still an active area of research. A recent example is CFCM. Similar to the above examples, it is based on a fully convolutional architecture consisting of an encoding and a decoding part. The core idea of the CFCM approach is to use a memory mechanism, implemented via convolutional LSTMs, for fusing features extracted from different layers of the encoder. Thereby, the convolutional LSTMs take the role of a coarse-to-fine focusing mechanism which first perceives the global context of the input data, as the deepest activations are fed to the inputs of the LSTM, and later processes\n",
        "fine-grained details. This happens when shallower, high-resolution features are considered. More details about convolutional LSTMs and this architectures are explained in the remainder of this exercise.\n",
        "\n",
        "The original implementation of CFCM, which has served as inspiration to create this exercise, is available on http://github.com/faustomilletari/CFCM-2D.\n",
        "\n",
        "<img src=cfcm_highlevel.png width=\"450\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZBgVTsZkKIt",
        "colab_type": "text"
      },
      "source": [
        "### CFCM with \"ladder\" ResNet as backbone architecture\n",
        "In this exercise, the encoding portion of CFCM is based\n",
        "on a standard ResNet architecture while decoding is implemented using convolutional\n",
        "LSTMs as explained above. \n",
        "\n",
        "![architecture](architecture.png)\n",
        "\n",
        "The ResNet encoder has 4 super-blocks which operate at different resolutions. Each super-block (each represented in figure with a different color) contain multiple ResNet blocks. The features resulting from each ResNet block are forwarded to the LSTM-based decoding path. This kind of pattern has been sometimes termed \"ladder\" network as it resembles a ladder where each step is a forward (long) skip connection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLLMuNGrkKIu",
        "colab_type": "text"
      },
      "source": [
        "For completeness, we include a picture representing the layer configuration of a ResNet block. In our implementation we also use Batch Normalization [Ref 5].\n",
        "<img src=block.png width=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhhwtJx9kKIv",
        "colab_type": "text"
      },
      "source": [
        "## Lung segmentation in X-Ray images\n",
        "X-Ray is a widely used modality for investigating the condition of the patient's lung. However, not only the lung is visible on these images. They contain the 2D projection of the entire anatomy, including bone and the heart. Determining the outline of the lung is therefore not an easy task, but necessary for many computer-aided procedures, visualization methods or for quantifying the extent of the lung.  \n",
        "\n",
        "### The Montgomery County X-ray Set\n",
        "The dataset employed in this exercise comprises 138 annotated posterior-anterior chest x-rays and has been acquired from the tuberculosis control program of the Department of Health and Human Services of Montgomery County, MD, USA. The set contains 80 normal cases and 58 abnormal cases with manifestations of tuberculosis including effusions and miliary patterns. We split this dataset in a training, validation and test set and use it to train, evaluate and infer using our DL approach. An example of images from this dataset is shown below.\n",
        "\n",
        "![X-ray Set](data.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHPKPJtAkKIx",
        "colab_type": "text"
      },
      "source": [
        "#  <span style=\"color:green\">Best practice for structuring code</span>\n",
        "\n",
        "## Code and exercise structure\n",
        "Structuring code to solve a machine learning problem to ensure both flexibility and adoption of best practices is not an easy task. In this exercise we try to incorporate some of the best principles that have emerged in popular recent python projects.\n",
        "\n",
        "With the introduction of modern frameworks such as tensorflow and pytorch, most of the processes around development of DL approaches have been standardized. During development of a typical project it is necessary to take care of only a handful of compartmentalized tasks such as:\n",
        "* DATA\n",
        "    1. Load data, standardize and augment it\n",
        "    2. Split dataset into batches and iterate through them\n",
        "* NETWORK\n",
        "    1. Define a network architecture as computational graph\n",
        "    2. Define suitable loss\n",
        "* OPTIMIZATION\n",
        "    1. Define optimization op\n",
        "    2. Implement training and validation loops\n",
        "    \n",
        "### Handling Data\n",
        "\n",
        "For **data** handling we define transforms in charge of loading, standardizing and modifying the dataset. Our transforms are chainable (stackable) such that data handling pipelines can be created. The dataset is stored in a python **dictionary** in order to allow this behaviour.\n",
        "\n",
        "**Transforms** are implemented by classes. In the constructor of these transforms (`__init__` method in python) we pass the parameters of the transform. We define the `__call__` method to accept only one user defined argument which is the dictionary containing data. \n",
        "\n",
        "```\n",
        "class ExampleTransform(object):\n",
        "    # this transformation adds a constant to the images of a dataset\n",
        "    \n",
        "    def __init__(self, constant):\n",
        "        # the constant that we add is passed as a parameter of this transform \n",
        "        self.constant = constant\n",
        "        \n",
        "    def __call__(self, data):\n",
        "        # data is a dictionary containing the dataset. we suppose it has a field 'images'\n",
        "        data['images'] = data['images'] + self.constant\n",
        "        \n",
        "        # the modified version of the data dictionary is returned as a result of the transform\n",
        "        return data \n",
        "```\n",
        "\n",
        "The example code above implements a simple transform that adds a constant to all of the images of the dataset. Other transforms, including those aiming at loading datasets from filesystem and actually inject new data into the 'data' dictionary can be implemented.\n",
        "\n",
        "In order to split the dataset into batches and iterate through these batches during training validation and potentially testing we need a batch iterator. This is implemented here through a python class which acts as a generator. That is, we can use our batch iterator object in a for loop to get batches that we can use during training. The batch iterator will return a dictionary containing data at each iteration. The batch iterator is also able to execute transforms (as defined above) both before and during iterating over the dataset. More details will be shown later in the exercise. \n",
        "\n",
        "### Network definition\n",
        "\n",
        "In the following sections of this exercise you will find the implementation of the network computational graph in tensorflow. A few helper functions have been defined in order to break down the implementation in more manageable portions and group together code that can be re-used.\n",
        "\n",
        "Loss layers can be defined very easily in tensorflow by implementing only the 'forward' computation of the loss and omitting the gradient implementation. This is possible thanks to the built-in automatic differentiation capabilities of tensorflow and other modern DL frameworks.\n",
        "\n",
        "### Fitting the networks parameters to the data\n",
        "\n",
        "In order to train, validate, and test the network we need to write the relevant code implementing the training, validation and testing loops (testing loop omitted here). The basic functionality of this code is to instantiate the network, define inputs (in terms of images and labels in this case), and finally instantiate batch iterators and optimizer.  \n",
        "At this point we can iterate (using a for loop) through the batches which can be fed to the network in order to optimize it for the task at hand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLTFFkMIkR2U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 982
        },
        "outputId": "5dea6810-b8ce-46cc-f277-353ea325a532"
      },
      "source": [
        "!pip install tensorflow==1.15"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 37kB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.12.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.9.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.30.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.2.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.18.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.2)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.1)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 30.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.34.2)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 36.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15) (49.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.2.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=c4250347c6e20fe455b9b12dd1786b6558bee2a182bc79a05bc022ede2d42e63\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, keras-applications, tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: tensorflow-estimator 2.3.0\n",
            "    Uninstalling tensorflow-estimator-2.3.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
            "  Found existing installation: tensorflow 2.3.0\n",
            "    Uninstalling tensorflow-2.3.0:\n",
            "      Successfully uninstalled tensorflow-2.3.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU2rVG4EkKIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In this section we import all the python packages used in this exercise\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import copy\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "\n",
        "from tensorflow.contrib import rnn\n",
        "from tensorflow.contrib import slim\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# set random seeds for reproducibility\n",
        "np.random.seed(4321)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HspdxUwnkKI8",
        "colab_type": "text"
      },
      "source": [
        "## Loading the dataset\n",
        "As previously explained, we understand all actions relative to data as **transformations**. Data loading is therefore understood as a transformation as well. A transformation, as already stated, is a callable (it is an object with a `__call__` method or a \"pointer\" to a function) which operates on a **dictionary** passed as input. **This dictionary contains the data**. The **transform changes** and/or adds to the content of this dictionary. \n",
        "\n",
        "In this exercise, loading the Montgomery XRay dataset is done by replacing/creating the fields 'images' and 'labels' of the data dictionary with a list of numpy tensors representing images. These images have been read from the filesystem itself (from a specific path where the dataset is stored) using scikit-image. Data is read when an object of class LungXRayDataset is called with a dictionary as argument. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWaIjHXokKJB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LungXRayDataset(object):\n",
        "    # This transformation loads data from the Montgomery County Lung XRay Dataset and updates the\n",
        "    # 'data' dictionary accordingly\n",
        "    \n",
        "    def __init__(self, images_path, labels_path):\n",
        "        # in this method we initialize the object 'LungXRayDataset' such that we know what paths\n",
        "        # we need to read in order to load images and labels\n",
        "        \n",
        "        label_left_path = os.path.join(labels_path, 'left')\n",
        "        label_right_path = os.path.join(labels_path, 'right')\n",
        "\n",
        "        image_names = [f for f in os.listdir(images_path) if 'png' in f]\n",
        "\n",
        "        self.image_path_list = [os.path.join(images_path, n) for n in image_names]\n",
        "        self.label_left_path_list = [os.path.join(label_left_path, n) for n in image_names]\n",
        "        self.label_right_path_list = [os.path.join(label_right_path, n) for n in image_names]\n",
        "        \n",
        "        assert len(self.image_path_list) == len(self.label_left_path_list)\n",
        "        assert len(self.label_left_path_list) == len(self.label_right_path_list)\n",
        "\n",
        "    def __call__(self, data):\n",
        "        # This method 'transforms' the 'data' dictionary by adding two fields to it, \n",
        "        # 'images' and 'labels', which contain a list of images and relative labels that \n",
        "        # can be used to train our DL approach\n",
        "        \n",
        "        images = []\n",
        "        labels = []\n",
        "        \n",
        "        for image_file, left_file, right_file in zip(self.image_path_list, self.label_left_path_list, self.label_right_path_list):\n",
        "            image = io.imread(image_file, as_gray=True).astype(np.float32) # read image\n",
        "            ll = io.imread(left_file, as_gray=True).astype(np.float32)  # read label for left lung\n",
        "            lr = io.imread(right_file, as_gray=True).astype(np.float32)  # read label for right lung\n",
        "            \n",
        "            # fuse label for left and right lungs. In this exercise we do only binary segmentation\n",
        "            label = np.zeros_like(image)\n",
        "            label[ll > 0] = 1\n",
        "            label[lr > 0] = 1\n",
        "            \n",
        "            image = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
        "            # append image and label to image and label lists\n",
        "            images.append(image)\n",
        "            labels.append(label)\n",
        "        \n",
        "        # update 'data' dictionary such that it can be further transformed and iterated\n",
        "        data['images'] = images\n",
        "        data['labels'] = labels\n",
        "        \n",
        "        return data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nFIcpD4kKJI",
        "colab_type": "text"
      },
      "source": [
        "## Transforming and standardizing the data \n",
        "Not all the images for this dataset have the same size and exhibit the same content. Some pictures have large areas that are padded with black pixels, and their size is not standard. Here we define additional transforms which aim to standardize the data by removing the black padding that is present in some pictures (CropActualImage transformation) and resizing the images to a conventional size (ResizeImage transformation). We also define another transformation which takes care of shuffling (ShuffleData) the dataset before batching it and feeding it to the network for training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-etjY7tkKJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CropActualImage(object):\n",
        "    # this transformation crops the images such that padding (with zeros) is cropped out\n",
        "    \n",
        "    def __call__(self, data):\n",
        "        # this method operates on 'data' dictionary. It replaces the fields \n",
        "        # 'images' and 'labels' with updated information\n",
        "        images_t = []\n",
        "        labels_t = []\n",
        "\n",
        "        for image, label in zip(data['images'], data['labels']):\n",
        "            non_zero_y = np.where(np.sum(image, axis=0) > 0)\n",
        "            non_zero_x = np.where(np.sum(image, axis=1) > 0)\n",
        "\n",
        "            image_t = image[np.min(non_zero_x):np.max(non_zero_x), np.min(non_zero_y):np.max(non_zero_y)]\n",
        "            label_t = label[np.min(non_zero_x):np.max(non_zero_x), np.min(non_zero_y):np.max(non_zero_y)]\n",
        "\n",
        "            images_t.append(image_t)\n",
        "            labels_t.append(label_t)\n",
        "\n",
        "        data['images'] = images_t\n",
        "        data['labels'] = labels_t\n",
        "        \n",
        "        return data"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLRG-GgFkKJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResizeImage(object):\n",
        "    # this transformation resizes the images and labels to a common size specified at init\n",
        "    \n",
        "    def __init__(self, size):\n",
        "        # a 2D iterable containing the desired image size need to be specified at init\n",
        "        self.size = size\n",
        "        \n",
        "    def __call__(self, data):\n",
        "        # this method operates on 'data' dictionary. It replaces the fields \n",
        "        # 'images' and 'labels' with updated information\n",
        "        data[\"images\"] = np.array(\n",
        "            [resize(i, (self.size[0], self.size[1]), preserve_range=True) for i in data['images']],\n",
        "            dtype=data['images'][0].dtype\n",
        "        )\n",
        "        data['labels'] = np.array(\n",
        "            [resize(i, (self.size[0], self.size[1]), preserve_range=True, order=0) for i in data['labels']],\n",
        "            dtype=data['labels'][0].dtype\n",
        "        )\n",
        "        \n",
        "        return data"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BACoNbHjkKJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ShuffleData(object):\n",
        "    # this transformation shuffles the dataset randomly\n",
        "    \n",
        "    def __init__(self, keys):\n",
        "        # keys is a list of strings containing the fields of \n",
        "        # the data dictionary that should be shuffled\n",
        "        self.keys = keys\n",
        "        \n",
        "    def __call__(self, data):\n",
        "        # this method operates on the 'data' dictionary and it replaces the \n",
        "        # fields of the dictionary contained in self.keys with shuffled \n",
        "        # versions of them. All fields are shuffled in the same way\n",
        "        data_length = len(data[self.keys[0]])\n",
        "        new_order = np.random.permutation(data_length)\n",
        "\n",
        "        for key in self.keys:\n",
        "            data[key] = data[key][new_order]\n",
        "\n",
        "        return data"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjzFSKC5kKJZ",
        "colab_type": "text"
      },
      "source": [
        "## Batching and iterating\n",
        "\n",
        "In this exercise the batch iterator takes care of the whole data loading/transformation/batching process. It gives us the ability to iterate through the dataset and specify the transformations that need to be applied to the data. Not all transformations need to be applied at the same time or at each loop iteration. We distinguish three cases:\n",
        "* transforms applied BEFORE starting iterating through the data, for example loading (transforms_before_iterating)\n",
        "* transforms applied to the whole training set at EACH EPOCH, for example shuffling (transforms_each_epoch)\n",
        "* transforms applied to EACH BATCH separately, for example augmentation (transforms_each_iteration)\n",
        "\n",
        "We implement here such object which has a method `__iter__` allowing us to use it as a generator (Eg. we can write `for batch in iterator: ...`) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUQ8xp7OkKJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BatchIterator(object):\n",
        "    # this object implements a batch iterator that can be used to iterate through a \n",
        "    # dataset during training or inference and to apply transforms to data dictionary\n",
        "    \n",
        "    def __init__(self, \n",
        "                 batch_size,\n",
        "                 keys,\n",
        "                 transforms_before_iterating=[], \n",
        "                 transforms_each_epoch=[], \n",
        "                 transforms_each_iteration=[]\n",
        "                ):\n",
        "        self.batch_size = batch_size\n",
        "        self.keys = keys\n",
        "        self.transforms_before_iterating = transforms_before_iterating\n",
        "        self.transforms_each_epoch = transforms_each_epoch\n",
        "        self.transforms_each_iteration = transforms_each_iteration\n",
        "        \n",
        "        self.data = {}  # at the beginning we have an empty dataset (data dictionary)\n",
        "        \n",
        "        for transform in self.transforms_before_iterating:\n",
        "            # for each transform in the list of transformation to do before iterating\n",
        "            # apply that transform to the data dictionary\n",
        "            self.data = transform(self.data)\n",
        "        \n",
        "    def __iter__(self):\n",
        "        # deep copy of the current data such that we can transform it \n",
        "        # as we like without influencing next epochs. \n",
        "        # this is just a working copy of self.data\n",
        "        data = copy.deepcopy(self.data) \n",
        "    \n",
        "        for transform in self.transforms_each_epoch:\n",
        "            # for each transform in the list of transformation to do at each epoch\n",
        "            # apply that transform to the data dictionary\n",
        "            data = transform(data)\n",
        "        \n",
        "        n_data = len(data[self.keys[0]])\n",
        "        n_batches = int(np.ceil(n_data / self.batch_size))\n",
        "        \n",
        "        for i in range(n_batches):\n",
        "            current_data = {}\n",
        "            for key in self.keys:\n",
        "                current_data[key] = data[key][i*self.batch_size:np.min([(i+1) * self.batch_size,  n_data])]\n",
        "                \n",
        "            for transform in self.transforms_each_iteration:\n",
        "                # for each transform in the list of transformation to on each batch\n",
        "                # apply that transform to the data dictionary\n",
        "                current_data = transform(current_data)\n",
        "        \n",
        "            yield current_data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qh3PmUc1kxZZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "1bb750b3-184e-40c7-b82e-b3230bd42ca7"
      },
      "source": [
        "##mount drive \n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MxB5jrIkKJf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# defining model parameters\n",
        "image_size = [256, 256]\n",
        "batch_size = 16\n",
        "num_epochs = 40\n",
        "learning_rate = 0.000033\n",
        "\n",
        "training_images_path = '../data/training/images'\n",
        "training_labels_path = '../data/training/labels'\n",
        "\n",
        "validation_images_path = '../data/validation/images'\n",
        "validation_labels_path = '../data/validation/labels'\n",
        "\n",
        "# declaring training batch iterator. the first 'transform' we do is loading the dataset. \n",
        "\n",
        "train_batch_iterator = BatchIterator(\n",
        "    batch_size=batch_size, \n",
        "    keys=['images', 'labels'], \n",
        "    transforms_before_iterating=[\n",
        "        LungXRayDataset(training_images_path, training_labels_path),  # first transform: load dataset (empty dataset > lung dataset)\n",
        "        CropActualImage(),  # second transform: remove padding (lung dataset with padding > lung dataset)\n",
        "        ResizeImage(image_size)  # third transform: resize images (lung dataset > resized lung dataset)\n",
        "    ],\n",
        "    transforms_each_epoch=[\n",
        "        ShuffleData(keys=['images', 'labels']),  # at each epoch we shuffle the dataset\n",
        "    ],\n",
        "    transforms_each_iteration=[],  # this allows us to add augmentations etc at each iteration, but we don't use this in this exercise.\n",
        ")\n",
        "\n",
        "valid_batch_iterator = BatchIterator(\n",
        "    batch_size=1, \n",
        "    keys=['images', 'labels'], \n",
        "    transforms_before_iterating=[\n",
        "        LungXRayDataset(validation_images_path, validation_labels_path),\n",
        "        CropActualImage(),  \n",
        "        ResizeImage(image_size)\n",
        "    ],\n",
        "    transforms_each_epoch=[], \n",
        "    transforms_each_iteration=[],  \n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqycQumEkKJk",
        "colab_type": "text"
      },
      "source": [
        "## Look at your data\n",
        "It is important to always inspect the data before deciding what method is appropriate to solve the problem at hand. Having a look at the data might mean to take into consideration statistics and distributions underlying the dataset, but in this case we are interested in visually inspecting it in order to be sure that it has been loaded and transformed correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iMYcihgkKJl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for data in train_batch_iterator:\n",
        "    plt.imshow(np.concatenate([np.squeeze(img) for img in np.split(data['images'], data['images'].shape[0], axis=0)], axis=1), cmap='gray')\n",
        "    plt.show()\n",
        "    plt.imshow(np.concatenate([np.squeeze(img) for img in np.split(data['labels'], data['labels'].shape[0], axis=0)], axis=1), cmap='jet')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsicIqdmkKJq",
        "colab_type": "text"
      },
      "source": [
        "#  <span style=\"color:green\">Going further: CFCM network architecture</span>\n",
        "\n",
        "## Defining the network architecture\n",
        "We now define the CFCM network architecture. In this example we show CFCM34 which is based on ResNet34 and uses LSTMs as a strategy to fuse features resulting from long skip connection at different resolutions. As shown in (Ref. ) the best performances on this dataset are obtained by a different, more complex, architecture but in order to reduce training time and computational load we find appropriate to use CFCM34 which is still capable of yielding very good results.\n",
        "\n",
        "### Utility functions, losses and model constants\n",
        "Here we define a few helper functions that are needed in order to create the network graph. We define a loss functiona and a scoring function based on the dice coefficient. We also define a few helper functions to facilitate the construction of the computational graph. \n",
        "\n",
        "### Dice loss/score formulation\n",
        "The dice coefficient measures the overlap between two (binary) contours and has been generalized and introduced as an objective function for FCNNs in [Ref. 3]. Since then it has been utilized in a number of scientific works and it is now very well established. The formulation used in this work is DICE=2 * (Gt * Pred) / (Gt^2 * Pred^2). This corresponds to Dice when both Gt and Pred are binary.\n",
        "\n",
        "other formulations such as DICE=2 * (Gt * Pred) / (Gt * Pred)\n",
        "\n",
        "have been proposed but have slightly different behaviours, especially when it comes to gradients. You can get more information about this topic in [Ref. 7]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoDon89EkKJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASE_NUM_KERNELS = 64\n",
        "EPS = 0.001\n",
        "\n",
        "# LOSS\n",
        "\n",
        "def dice(prediction, truth):\n",
        "    return 2.0 * tf.reduce_sum(prediction * truth, axis=[1, 2, 3]) / (tf.reduce_sum((prediction ** 2 + truth ** 2), [1, 2, 3]) + EPS)\n",
        "\n",
        "def dice_score(prediction, truth):\n",
        "    dc = dice(prediction, truth)\n",
        "    return tf.reduce_mean(dc, axis=0)\n",
        "\n",
        "def dice_loss(prediction, truth):\n",
        "    dc = dice(prediction, truth)\n",
        "    return tf.reduce_mean(1.0 - dc, axis=0)\n",
        "\n",
        "# COMMON BLOCKS/LAYERS\n",
        "\n",
        "def batch_norm_relu(inputs, is_training):\n",
        "    #net = tf.contrib.layers.batch_norm(inputs, is_training=is_training, zero_debias_moving_mean=True, decay=0.9)\n",
        "    net = tf.nn.relu(inputs)\n",
        "    return net\n",
        "\n",
        "\n",
        "def conv2d_transpose(inputs, output_channels, kernel_size):\n",
        "    return slim.conv2d_transpose(\n",
        "        inputs,\n",
        "        num_outputs=output_channels,\n",
        "        kernel_size=kernel_size,\n",
        "        stride=2,\n",
        "    )\n",
        "\n",
        "\n",
        "def conv2d_fixed_padding(inputs, filters, kernel_size, stride):\n",
        "    return slim.conv2d(\n",
        "        inputs,\n",
        "        filters,\n",
        "        kernel_size,\n",
        "        stride=stride,\n",
        "        padding=('SAME' if stride == 1 else 'VALID'),\n",
        "        activation_fn=None\n",
        "    )\n",
        "\n",
        "\n",
        "def building_block(inputs, filters, is_training, projection_shortcut, stride):\n",
        "    # this implements one block of the ResNet (\n",
        "    # convolutional operations and skip connection included)\n",
        "    \n",
        "    shortcut = inputs\n",
        "    inputs = batch_norm_relu(inputs, is_training)\n",
        "    # The projection shortcut should come after the first batch norm and ReLU\n",
        "    # since it performs a 1x1 convolution.\n",
        "    if projection_shortcut is not None:\n",
        "        shortcut = projection_shortcut(inputs)\n",
        "\n",
        "    inputs = conv2d_fixed_padding(inputs=inputs, filters=filters, kernel_size=3, stride=stride)\n",
        "\n",
        "    inputs = batch_norm_relu(inputs, is_training)\n",
        "\n",
        "    inputs = conv2d_fixed_padding(inputs=inputs, filters=filters, kernel_size=3, stride=1)\n",
        "\n",
        "    return inputs + shortcut\n",
        "\n",
        "\n",
        "\n",
        "def block_layer_compressing(inputs, filters, blocks, stride, is_training, name):\n",
        "    def projection_shortcut(inputs):\n",
        "        return conv2d_fixed_padding(inputs=inputs, filters=filters, kernel_size=1, stride=stride)\n",
        "\n",
        "    # Only the first block per block_layer uses projection_shortcut and strides\n",
        "    inputs = building_block(inputs, filters, is_training, projection_shortcut, stride)\n",
        "\n",
        "    layers_outputs = [inputs]\n",
        "\n",
        "    for i in range(1, blocks):\n",
        "        inputs = building_block(inputs, filters, is_training, None, 1)\n",
        "\n",
        "        layers_outputs.append(tf.nn.relu(inputs))\n",
        "\n",
        "    return tf.identity(inputs, name), layers_outputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6sVqt-akKJw",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional LSTMs\n",
        "At the core of the feature fusion strategy of CFCM, there is a memory mechanism which is implemented via a convolutional LSTM. The decoder treats each block of the ResNet encoder as a single time-step. As discussed above, CFCM forwards the outputs of these blocks to the decoding path where the features are processed through LSTM cells. To this end, convolutional LSTMs are employed. Convolutional LSTMs have the capability of selectively updating their\n",
        "internal states at each step depending on the result of a convolution.\n",
        "\n",
        "There are multiple reasons why it is necessary to use a convolutional LSTM in this case:\n",
        "* Number of parameters: using classic LSTMs we would need to resort to a fully connected layer to produce the features for the LSTM gates. This would result in a parameter explosion due to the high dimensionality of the input features and therefore impair learning.\n",
        "* We deal with images: convolutions are an appropriate choice when information is spatially localized such as in the case of images.\n",
        "* Speed and memory: Less parameters means less operations and memory. Using classic LSTMs would be prohibitive when it comes to computational time and memory requirements.  \n",
        "\n",
        "Here you can see a graphical representation of the convolutional LSTM employed in this work:\n",
        "<img src=conv_lstm_basic.png width=\"600\">\n",
        "\n",
        "The input features, which in the CFCM architecture are forwarded from the encoding path, are represented in orange. These features are the results of convolutions and pooling operations on the network inputs. At each time-step a 4D input is presented to the LSTM. The frist dimension is the batch size, the next two dimensions are width and height and the last dimension is the channels. The number of channels can be large. \n",
        "\n",
        "The hidden state and cell state are part of the LSTM architecture. Both hidden and cell state are propagated through the sequence. The hidden state is also the output of the LSTM cell. The hidden state/output is never computed directly from the previous hidden state but it is computed from the cell state instead. There is no direct connection between hidden state from a previous step and the output at the next.\n",
        "\n",
        "As shown in figure each time step makes use of three feature sets: inputs, hidden and cell state. Inputs are concatenated with the hidden state. A convolution is performed and its result is used to (1) pass a part of the information stored in the cell state through the forget gate; (2) compute new features which contribute to the cell state after being (3) decimated; (4) compute a new hidden state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBQ07Z3WkKJx",
        "colab_type": "text"
      },
      "source": [
        "### (Convolutional) LSTM in action\n",
        "The main difference between classic LSTMs and convolutional ones is the usage of a convolution operation to obtain the update features for the cell as well as the gating signals for the forget, input and output gates.\n",
        "\n",
        "To understand how both classic and convolutional LSTMs work we will break down their behaviour for one step of a sequence. In the following drawing we have unrolled the LSTM across two sequence steps and have highlighted with a red box the operations and components relative to one of the two steps. \n",
        "\n",
        "<img src=conv_lstm_time.png width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Iw_bOyokKJ1",
        "colab_type": "text"
      },
      "source": [
        "The features fed into our LSTM (at time t) are:\n",
        "1. the hidden state at time t (at time zero an initialization is provided instead)\n",
        "2. the cell state at time t (at time zero an initialization is provided instead)\n",
        "3. the input at time t\n",
        "\n",
        "<img src=conv_lstm_step1.png width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp9Z-nuLkKJ2",
        "colab_type": "text"
      },
      "source": [
        "The inputs (at time t) are concatenated with the hidden state (at time t) and convolved with a kernel having 4xC channels which creates 4 sets of features having C channels each. These features are used to:\n",
        "1. drive the LSTM gates (forget, input, output) \n",
        "2. produce an update signal for the LSTM state consisting of features computed from input and hidden state\n",
        "\n",
        "<img src=conv_lstm_step2.png width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEEedBSykKJ3",
        "colab_type": "text"
      },
      "source": [
        "The features that are used to drive the three gates of the LSTM (forget, input and output) use a sigmoid non-linearity. In this way the output will be always between [0,1]. The update signal uses a different kind of non-linearity which can be in general chosen by the user and in the case of CFCM is a ReLu non-linearity. \n",
        "\n",
        "<img src=conv_lstm_step3.png width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtMpiURXkKJ7",
        "colab_type": "text"
      },
      "source": [
        "The cell state is multiplied by the forget gate. this means that some of the information contained within the cell state is decimated (removed) by multiplying with values close to zero while some other information is kept since it's multiplied with values close to one. \n",
        "\n",
        "The input gate decimates (removes) information belonging to the update signal computed in the previous step. In this way only some of the information computed via convolution from inputs and hidden state contributes to the cell state update.\n",
        "\n",
        "The output gate will be used to choose which information belonging to the cell state (after activation) will be used to create the new hidden state. \n",
        "\n",
        "<img src=conv_lstm_step4.png width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZweLGzzDkKJ8",
        "colab_type": "text"
      },
      "source": [
        "Information belonging to the cell state after (forget) gating and new information from the update signal after (input) gating, are fused together via summation.\n",
        "\n",
        "<img src=conv_lstm_step5.png width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSiuK2P4kKJ9",
        "colab_type": "text"
      },
      "source": [
        "In this way we obtain a new cell state (cell state at time t+1) which we can also use to obtain a new hidden state.\n",
        "\n",
        "<img src=conv_lstm_step6.png width=\"700\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6yW7lipkKJ-",
        "colab_type": "text"
      },
      "source": [
        "The new hidden state (at step t+1) is obtained by (output) gating the cell state (at time t+1) after ReLu non linearity. \n",
        "\n",
        "<img src=conv_lstm_step7.png width=\"700\">"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izYFb0IUkKJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IMPLEMENTATION OF CONV LSTM\n",
        "class ConvLSTMCell(object):\n",
        "    def __init__(self, shape, filter_size, num_features, forget_bias=1.0, activation=tf.nn.tanh, scope=''):\n",
        "        self.shape = shape\n",
        "        self.filter_size = filter_size\n",
        "        self.num_features = num_features\n",
        "        self.forget_bias = forget_bias\n",
        "        self.activation = activation\n",
        "        self.num_units = num_features\n",
        "        self.scope = scope\n",
        "    \n",
        "    def zero_state(self, batch_size, dtype):\n",
        "        init_state = tf.zeros([batch_size, self.shape[0], self.shape[1], self.num_features * 2])\n",
        "        return init_state\n",
        "\n",
        "    @property\n",
        "    def state_size(self):\n",
        "        return 2 * self.num_units\n",
        "\n",
        "    @property\n",
        "    def output_size(self):\n",
        "        return self.num_units\n",
        "    \n",
        "    def __call__(self, inputs, state, scope=None):\n",
        "        with tf.variable_scope(scope or type(self).__name__):\n",
        "            cell, hidden = tf.split(axis=3, num_or_size_splits=2, value=state)\n",
        "            \n",
        "            features = lstm_conv(\n",
        "                inputs=[inputs, hidden], \n",
        "                filter_size=self.filter_size, \n",
        "                num_features=self.num_features * 4,\n",
        "                use_bias=True,\n",
        "                init_bias=0.0,\n",
        "                scope=self.scope + 'conv'\n",
        "            )\n",
        "\n",
        "            # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n",
        "            i, j, f, o = tf.split(axis=3, num_or_size_splits=4, value=features)\n",
        "\n",
        "            new_cell = (cell * tf.nn.sigmoid(f + self.forget_bias) + tf.nn.sigmoid(i) *\n",
        "                     self.activation(j))\n",
        "            \n",
        "            new_hidden = self.activation(new_cell) * tf.nn.sigmoid(o)\n",
        "\n",
        "            new_state = tf.concat(axis=3, values=[new_cell, new_hidden])\n",
        "            \n",
        "            return new_hidden, new_state\n",
        "\n",
        "\n",
        "def lstm_conv(inputs, filter_size, num_features, use_bias, init_bias=0.0, scope='Conv'):\n",
        "    n_in_chan = 0\n",
        "    shapes = [elem.get_shape().as_list() for elem in inputs]\n",
        "    \n",
        "    for shape in shapes:\n",
        "        n_in_chan += shape[3]\n",
        "\n",
        "    dtype = inputs[0].dtype\n",
        "\n",
        "    with tf.variable_scope(scope):\n",
        "        weights = tf.get_variable(\n",
        "            \"weights\", [filter_size[0], filter_size[1], n_in_chan, num_features], dtype=dtype)\n",
        "\n",
        "        res = tf.nn.conv2d(tf.concat(axis=3, values=inputs), weights, strides=[1, 1, 1, 1], padding='SAME')\n",
        "        \n",
        "        if not use_bias:\n",
        "            return res\n",
        "        \n",
        "        additive_bias = tf.get_variable(\n",
        "            \"bias\", \n",
        "            [num_features],\n",
        "            dtype=dtype,\n",
        "            initializer=tf.constant_initializer(\n",
        "                init_bias, \n",
        "                dtype=dtype\n",
        "            )\n",
        "        )\n",
        "        \n",
        "    return res + additive_bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJjVxnr4kKKD",
        "colab_type": "text"
      },
      "source": [
        "## Coarse to Fine Context Memory (CFCM) based on ResNet34\n",
        "\n",
        "We are now ready to implement the CFCM architecture making use of the convolutional LSTM code we have implemented above and the helper functions that we have defined earlier. The following code implements the computational graph of CFCM based on ResNet34. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut-EUcw-kKKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CFCM34(object):\n",
        "    layers = [3, 4, 6, 3]  # description of layers arrangement (ResNet 34)\n",
        "    \n",
        "    def __init__(self, num_classes, data_format='channels_last'):\n",
        "        self.num_classes = num_classes\n",
        "        self.data_format = data_format\n",
        "    \n",
        "    def __call__(self, inputs, is_training):\n",
        "        base_num_kernels = BASE_NUM_KERNELS\n",
        "\n",
        "        inputs = conv2d_fixed_padding(inputs=inputs, filters=base_num_kernels, kernel_size=7, stride=1)\n",
        "\n",
        "        inputs = tf.layers.max_pooling2d(inputs=inputs, pool_size=2, strides=2, padding='SAME')\n",
        "\n",
        "        output_b1, output_list_b1 = block_layer_compressing(\n",
        "            inputs=inputs,\n",
        "            filters=base_num_kernels,\n",
        "            blocks=self.layers[0],\n",
        "            stride=1,\n",
        "            is_training=is_training,\n",
        "            name='block_layer1'\n",
        "        )\n",
        "\n",
        "        output_b1 = tf.layers.max_pooling2d(\n",
        "            inputs=output_b1, \n",
        "            pool_size=2, \n",
        "            strides=2, \n",
        "            padding='SAME',\n",
        "            data_format=self.data_format\n",
        "        )\n",
        "\n",
        "        output_b2, output_list_b2 = block_layer_compressing(\n",
        "            inputs=output_b1, \n",
        "            filters=base_num_kernels * 2, \n",
        "            blocks=self.layers[1],\n",
        "            stride=1, \n",
        "            is_training=is_training, \n",
        "            name='block_layer2'\n",
        "        )\n",
        "\n",
        "        output_b2 = tf.layers.max_pooling2d(\n",
        "            inputs=output_b2, \n",
        "            pool_size=2, \n",
        "            strides=2, \n",
        "            padding='SAME',\n",
        "            data_format=self.data_format\n",
        "        )\n",
        "\n",
        "        output_b3, output_list_b3 = block_layer_compressing(\n",
        "            inputs=output_b2, \n",
        "            filters=base_num_kernels * 4,  \n",
        "            blocks=self.layers[2],\n",
        "            stride=1, \n",
        "            is_training=is_training, \n",
        "            name='block_layer3'\n",
        "        )\n",
        "\n",
        "        output_b3 = tf.layers.max_pooling2d(\n",
        "            inputs=output_b3, \n",
        "            pool_size=2, \n",
        "            strides=2, \n",
        "            padding='SAME',\n",
        "            data_format=self.data_format\n",
        "        )\n",
        "\n",
        "        output_b4, output_list_b4 = block_layer_compressing(\n",
        "            inputs=output_b3, \n",
        "            filters=base_num_kernels * 8, \n",
        "            blocks=self.layers[3],\n",
        "            stride=1, \n",
        "            is_training=is_training, \n",
        "            name='block_layer4'\n",
        "        )\n",
        "\n",
        "        # lstm - decoding path\n",
        "\n",
        "        initial_hidden = tf.zeros_like(output_b4)\n",
        "        initial_cell = tf.zeros_like(output_b4)\n",
        "\n",
        "        initial_state = tf.concat([initial_cell, initial_hidden], axis=3)\n",
        "\n",
        "        shape = [output_b4.get_shape().as_list()[1], output_b4.get_shape().as_list()[2]]\n",
        "        \n",
        "        lstm_b4 = ConvLSTMCell(shape, [3, 3], num_features=base_num_kernels * 8, scope='lstm_b4',\n",
        "                                    activation=tf.nn.relu)\n",
        "\n",
        "        _, state = rnn.static_rnn(lstm_b4, output_list_b4[::-1], initial_state=initial_state, dtype=tf.float32)\n",
        "\n",
        "        state = conv2d_transpose(state, kernel_size=2, output_channels=base_num_kernels * 8)\n",
        "\n",
        "        shape = [output_b3.get_shape().as_list()[1], output_b3.get_shape().as_list()[2]]\n",
        "        lstm_b3 = ConvLSTMCell(shape, [3, 3], num_features=base_num_kernels * 4, scope='lstm_b3',\n",
        "                                    activation=tf.nn.relu)\n",
        "\n",
        "        _, state = rnn.static_rnn(lstm_b3, output_list_b3[::-1], initial_state=state, dtype=tf.float32)\n",
        "\n",
        "        state = conv2d_transpose(state, kernel_size=2, output_channels=base_num_kernels * 4)\n",
        "\n",
        "        shape = [output_b2.get_shape().as_list()[1], output_b2.get_shape().as_list()[2]]\n",
        "        lstm_b2 = ConvLSTMCell(shape, [3, 3], num_features=base_num_kernels * 2, scope='lstm_b2',\n",
        "                                    activation=tf.nn.relu)\n",
        "\n",
        "        _, state = rnn.static_rnn(lstm_b2, output_list_b2[::-1], initial_state=state, dtype=tf.float32)\n",
        "\n",
        "        state = conv2d_transpose(state, kernel_size=2, output_channels=base_num_kernels * 2)\n",
        "\n",
        "        shape = [output_b1.get_shape().as_list()[1], output_b1.get_shape().as_list()[2]]\n",
        "        lstm_b1 = ConvLSTMCell(shape, [3, 3], num_features=base_num_kernels, scope='lstm_b1',\n",
        "                                    activation=tf.nn.relu)\n",
        "\n",
        "        output, state = rnn.static_rnn(lstm_b1, output_list_b1[::-1], initial_state=state, dtype=tf.float32)\n",
        "\n",
        "        hidden = conv2d_transpose(output[-1], kernel_size=2, output_channels=base_num_kernels)\n",
        "\n",
        "        conv_final = conv2d_fixed_padding(inputs=hidden, filters=base_num_kernels, kernel_size=3, stride=1)\n",
        "        \n",
        "        conv_final = tf.nn.relu(conv_final)\n",
        "\n",
        "        outputs = conv2d_fixed_padding(inputs=conv_final, filters=self.num_classes, kernel_size=3, stride=1)\n",
        "\n",
        "        return outputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxxWpyyCkKKL",
        "colab_type": "text"
      },
      "source": [
        "## Implementation of training + validation loops\n",
        "We implement here both the training and validation loops as well as the code necessary to open the tensorflow session and instantiate/initialize the graph. We define here input placeholders, loss, and the optimization op (fitting_op) which is essential to train the network. We loop over the training and validation sets for a number of epochs and plot results for both training and validation in order to give you a sense of how the network training progresses over time. Training + validation can be run by executing the cell after the next. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpqLLZcRkKKN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_training_and_validation(network):\n",
        "    with tf.Graph().as_default() as g:\n",
        "        tf.set_random_seed(777)\n",
        "        with tf.Session() as sess:\n",
        "            # ResNet34-based Network WITHOUT COARSE TO FINE CONTEXT MEMORY. <HINT> edit the code to use CFCM34\n",
        "            SegResNet = network(num_classes=1)\n",
        "\n",
        "            # Placeholders for the inputs\n",
        "            images = tf.placeholder(shape=[None, 256, 256, 1], dtype=tf.float32)\n",
        "            labels = tf.placeholder(shape=[None, 256, 256, 1], dtype=tf.float32)\n",
        "            is_training = tf.placeholder(shape=(), dtype=tf.bool)\n",
        "\n",
        "            # Network output\n",
        "            prediction = tf.sigmoid(SegResNet(images, is_training))\n",
        "\n",
        "            # Loss function\n",
        "            loss = dice_loss(prediction, labels)\n",
        "            \n",
        "            # Score function (binarize the prediction for accurate results!)\n",
        "            score = dice_score(tf.cast(prediction > 0.5, dtype=tf.float32), labels)\n",
        "\n",
        "            # Optimizer\n",
        "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "\n",
        "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "\n",
        "            with tf.control_dependencies(extra_update_ops):\n",
        "                fitting_op = optimizer.minimize(loss)\n",
        "\n",
        "            # Initialization\n",
        "            init_op = tf.global_variables_initializer()\n",
        "            sess.run(init_op)\n",
        "\n",
        "            training_scores = []\n",
        "            valid_scores = []\n",
        "            \n",
        "            training_epochs_axis = []\n",
        "            valid_epochs_axis = []\n",
        "\n",
        "            for i in range(num_epochs):\n",
        "                print('---------- TRAINING EPOCH {} out of {} ----------'.format(i, num_epochs))\n",
        "\n",
        "                epoch_training_scores = []\n",
        "                epoch_valid_scores = []\n",
        "                \n",
        "                training_epochs_axis.append(i)\n",
        "\n",
        "                for data in train_batch_iterator:\n",
        "                    _, curr_training_score, curr_prediction = sess.run(\n",
        "                        [fitting_op, score, prediction], \n",
        "                        feed_dict={\n",
        "                            images: data['images'][..., np.newaxis],\n",
        "                            labels: data['labels'][..., np.newaxis],\n",
        "                            is_training: True,\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "                    epoch_training_scores.append(curr_training_score)\n",
        "\n",
        "                    plt.imshow(np.concatenate([np.squeeze(img) for img in np.split(data['images'], data['images'].shape[0], axis=0)], axis=1), cmap='gray')\n",
        "                    plt.show()\n",
        "                    plt.imshow(np.concatenate([np.squeeze(img) for img in np.split(data['labels'], data['labels'].shape[0], axis=0)], axis=1), cmap='jet')\n",
        "                    plt.show()\n",
        "                    plt.imshow(np.concatenate([np.squeeze(img) for img in np.split(curr_prediction, curr_prediction.shape[0], axis=0)], axis=1), cmap='jet')\n",
        "                    plt.show()\n",
        "\n",
        "                training_scores.append(np.mean(epoch_training_scores))\n",
        "                print('Average TRAINING score for epoch {}: {}'.format(i, np.mean(epoch_training_scores)))\n",
        "                \n",
        "                # running validation only after the network attains decent performances (in the interest of time)\n",
        "                \n",
        "                if training_scores[-1] > 0.92:\n",
        "                    print('---------- VALIDATION EPOCH {} out of {} ----------'.format(i, num_epochs))\n",
        "                    \n",
        "                    valid_epochs_axis.append(i)\n",
        "                    \n",
        "                    for data in valid_batch_iterator:\n",
        "\n",
        "                        curr_valid_score, curr_prediction = sess.run(\n",
        "                            [score, prediction], \n",
        "                            feed_dict={\n",
        "                                images: data['images'][..., np.newaxis],\n",
        "                                labels: data['labels'][..., np.newaxis],\n",
        "                                is_training: False,\n",
        "                            }\n",
        "                        )\n",
        "\n",
        "                        epoch_valid_scores.append(curr_valid_score)\n",
        "\n",
        "                        plt.imshow(np.concatenate([np.squeeze(img) for img in np.split(data['images'], data['images'].shape[0], axis=0)], axis=1), cmap='gray')\n",
        "                        plt.show()\n",
        "                        plt.imshow(np.concatenate([np.squeeze(img > 0.5) for img in np.split(data['labels'], data['labels'].shape[0], axis=0)], axis=1), cmap='gray')\n",
        "                        plt.show()\n",
        "                        plt.imshow(np.concatenate([np.squeeze(img > 0.5) for img in np.split(curr_prediction, curr_prediction.shape[0], axis=0)], axis=1), cmap='gray')\n",
        "                        plt.show()\n",
        "\n",
        "                    valid_scores.append(np.mean(epoch_valid_scores))\n",
        "                    print('Average VALIDATION score for epoch {}: {}'.format(i, np.mean(epoch_valid_scores)))\n",
        "                \n",
        "    return training_scores, valid_scores, training_epochs_axis, valid_epochs_axis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "gxAKA9qikKKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We run training + validation by executing this cell\n",
        "training_scores, valid_scores, train_axis, valid_axis = run_training_and_validation(CFCM34)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0O2CUbnkKKV",
        "colab_type": "text"
      },
      "source": [
        "## Plotting scores\n",
        "We now plot the training and validation scores in terms of Dice coefficient (the higher, the better). More informations about the results and more experiments can be found in the original paper [Ref. 2]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNP9jCkakKKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(train_axis, training_scores, 'b', valid_axis, valid_scores, 'r')\n",
        "red_patch = mpatches.Patch(color='red', label='Validation')\n",
        "blue_patch = mpatches.Patch(color='blue', label='Training')\n",
        "plt.legend(handles=[red_patch, blue_patch])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQBdX4qBkKKZ",
        "colab_type": "text"
      },
      "source": [
        "## <span style=\"color:green\">Conclusions</span>\n",
        "In this exercise you have implemented CFCM which is an interesting alternative to classic feature fusion for long skip connections in fully convolutional neural networks (FCNN) applied to medical image segmentation. \n",
        "The multi-scale feature integration of CFCM is achieved via LSTMs which process and aggregate features extracted at different resolutions and having different receptive fields. The architecture is based on ResNet34 and the training strategy is similar to the one adopted by more classical and widespread FCNN methods: images are fed to the algorithm together with their labels and a loss function (dice loss in this case) is optimized in a fully supervised manner.\n",
        "\n",
        "As a result of carrying on this exercise you have familiarized with concepts such as \n",
        "* fully convolutional neural networks\n",
        "* residual networks\n",
        "* long/short skip connections to improve image segmentation\n",
        "* batch normalization\n",
        "* receptive field\n",
        "* recurrent neural networks (RNN) \n",
        "* (convolutional) long short term memory cells (LSTMs)\n",
        "\n",
        "You have also familiarized with a simple yet powerful way to structure deep learning project allowing to modularize basic tasks such as data loading, manipulation and augmentation, in a way that is scalable to larger project and compatible with practices adopted by frameworks such as pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry4JKwlekKKa",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "* Ref. 1: *Milletari, F., Rieke, N., Baust, M., Esposito, M. and Navab, N., 2018. CFCM: Segmentation via Coarse to Fine Context Memory. arXiv preprint arXiv:1806.01413.*\n",
        "* Ref. 2: *Ronneberger, O., Fischer, P. and Brox, T., 2015, October. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention (pp. 234-241). Springer, Cham.*\n",
        "* Ref. 3: *Milletari, F., Navab, N. and Ahmadi, S.A., 2016, October. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 3D Vision (3DV), 2016 Fourth International Conference on (pp. 565-571). IEEE.*\n",
        "* Ref. 4: *Laina, I., Rieke, N., Rupprecht, C., Vizcaíno, J. P., Eslami, A., Tombari, F., and Navab, N. 2017. Concurrent segmentation and localization for tracking of surgical instruments. In International conference on medical image computing and computer-assisted intervention (pp. 664-672). Springer*\n",
        "* Ref. 5: *Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167*\n",
        "* Ref. 6: *Drozdzal, M., Vorontsov, E., Chartrand, G., Kadoury, S. and Pal, C., 2016. The importance of skip connections in biomedical image segmentation. In Deep Learning and Data Labeling for Medical Applications (pp. 179-187). Springer, Cham*\n",
        "* Ref. 7: *Milletari, F., 2018. Hough Voting Strategies for Segmentation, Detection and Tracking (Doctoral dissertation, Universität München)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9TopGYIkKKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}